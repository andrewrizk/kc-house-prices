---
title: "King County, House Price Prediction"
author: "Andrew Rizk"
date: "26-05-2019"
output:
  html_document:
    df_print: paged
    toc: yes
    highlight: tango
  pdf_document:
    fig_caption: yes
    fig_height: 4.5
    fig_width: 7.5
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc: yes
  word_document:
    fig_caption: yes
    fig_height: 4.5
    fig_width: 7.5
    highlight: tango
    toc: yes
---
![](https://static-frm.ie.edu/exponential-learning/wp-content/uploads/sites/22/2017/06/Logo_Human-Sciences.png)

#### Github link
<https://github.com/andrewrizk/kc-house-price>

#### 1. Introduction
In this following practice, we will attempt to predict house prices in King Country in Seattle based on the House Prices Dataset on Kaggle: <https://www.kaggle.com/harlfoxem/housesalesprediction>. It includes homes sold between May 2014 and May 2015.

The dataset presents different features of the houses that variably change the predictions. The data will require some work for cleaning and preprocessing to be ready for modeling.

The steps of this practice will involve:
1 - Data cleaning and preprocessing.
2 - Exploratory data analysis.
3 - Feature Engineering: this will involve transforming variables, creating new variables, and eliminating variables in favor of our predictions.
4 - Correlations
5 - Removing Outliers/Scaling/Adjusting Skewness
6 - Modeling:
    a. Linear Regressions
    b. Ridge Regression
    c. Lasso Regression
    d. XGBoost
    e. Random Forest
7 - Model Comparison

* Final model was selected as XGBoost with a MAPE of 0.114 *

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)

```

#### 2. Load Libraries
```{r Libraries to be used for this project, eval=TRUE, Warnig=FALSE}
library(data.table)
library(DataExplorer)
library(ggplot2)
library(plyr)
library(tidyr)
library(dplyr)
library(knitr)     
library(moments)   
library(e1071)     
library(glmnet)    
library(caret)
library(corrplot)
library(ggloop)
library(leaflet)
library(RColorBrewer)
library(stringr)
library(randomForest)
library(xgboost)
library(rpart)
library(partykit)
library(rpart.plot)
library(ranger)
```

#### 2. Load Datasets
We will first load the datasets and create a price column for the test set so we can combine them together for further preprocessing.
```{r Datasets}
# We will load the original datasets and keep them untouched if needed for later purposes
original_train <- read.csv("house_price_train.csv", header = T)
original_test <- read.csv("house_price_test.csv", header = T)

# Loading datasets
house_train <- read.csv("house_price_train.csv", header = T)
house_test <- read.csv("house_price_test.csv", header = T)

# Since the test set has no price column, we will create one before combining both
house_test$price <- NA

# We will add a column with a factor of train/test to help with the dataset splitting further
house_train$train_test <- "train"
house_test$train_test <- "test"

# Combining both datasets
house_dataset <- rbind(house_train, house_test)
cat("The train set has ", dim(house_train)[1], "rows and ", dim(house_train)[2], "columns, ")
cat("The test set has ", dim(house_test)[1], "rows and ", dim(house_test)[2], "columns, ")
cat("The combined dataset has ", dim(house_dataset)[1], "rows and ", dim(house_dataset)[2], "columns")

```

#### 3. Feature Removal
We can identify some features that don't add any value to the dataset due to their nature.
The variable 'Id' is a key and will not have any impact in the final predicion. Other features will be removed later after the feature engineering process.
```{r Feature removal}
house_dataset <- house_dataset[,-which(names(house_dataset) == "id")]
```

#### 4. NA Discovery
We need to clean NA values in all the columns and impute null values (if present).
First, we explore which columns have missing values.

```{r}
# Using DataExplorer Library
plot_missing(house_dataset)

na.cols <- which(colSums(is.na(house_dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
sort(colSums(sapply(house_dataset[na.cols], is.na)), decreasing = TRUE)

```
Therefore no missing values are present in the dataset and we can continue with feature engineering.

#### 5. Data Structure
Discovering the data structure and a quantitative summary of the variables.
```{r Dataset Structure and Features}
# visualizing the dataset to know the structure and components of each variable
str(house_dataset)
summary(house_dataset)
```

#### 6. Factorize Numerical to Categories
There are some numerical features that should be factorized as categorical ones, in order to gasp correctly the information inside those columns.
If we analyze some features we will see that we have categorical values encoded as numeric. We will transform some of those into factors since they wont be used in any calculation. Note that some features will be factorized later after the EDA process.
```{r}
#Specifying the columns that are counted as numeric
columns <- c("condition", "grade", "waterfront", "zipcode")
house_dataset[columns] <- lapply(house_dataset[columns], factor)
```

#### 7. EDA

```{r Structure and Target Variable}
#Structure of dataset after factorizing
str(house_dataset)

#Histogram to see the distribution of the house prices
ggplot(house_train,aes(x=price))+geom_density(fill="dodgerblue3") + scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

# We notice the skewness of the prices to the right so we can do a log transformation to the price
ggplot(house_train,aes(x= log10(price)))+geom_histogram(fill="dodgerblue3") + scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

# Using the log transormation we notice that the prices mostly range from 5 to 6.5 million

```

We will now create a new column to categorize the prices into 4 categories (Cheap, Medium, Expensive, Very Expensive) and visualize the results using leaflet. Though since the code below really slows down my Rmarkdown, I decided to comment it out.
```{r}
#We will now create a new column to categorize the prices into 4 categories (Cheap, Medium, Expensive, Very Expensive) and visualize the results using leaflet. Though since the code below really slows down my Rmarkdown, I decided to comment it out.

# house_train_temp <- house_train
# quantile(house_train_temp$price)
# 
# #Using the above quantiles we can categorize the houses to 4 categories
# house_train_temp <- house_train_temp %>% mutate(price_category = ifelse(price <= 320000, "Cheap",
#                                           ifelse(price > 320000 &
#                                                    price <= 450000, "Average",
#                                                  ifelse(price > 450000 & 
#                                                           price <= 645500, "Expensive",
#                                                         ifelse(price > 645000, "Very Expensive", NA)))))
# 
# 
# sample <-house_train_temp[sample(nrow(house_train_temp), 500), c("long", "lat", "price_category")] 
# 
# getColor <- function(house_train_temp) {
#   sapply(house_train_temp$price_category, function(price_category) {
#     if(as.character(price_category) == "Cheap") {
#       "green"
#     } else if(as.character(price_category) == "Average") {
#       "yellow"
#     } else if(as.character(price_category) == "Expensive") {
#       "orange"
#     } else {
#       "red"
#     } })
# }
# 
# icons <- awesomeIcons(
#   icon = 'icon',
#   iconColor = 'black',
#   library = 'ion',
#   markerColor = getColor(house_train_temp)
# )

# leaflet(data = house_train_temp) %>% addTiles() %>% setView(lng = -122, lat = 47.8, zoom = 7.5)%>%
#   addAwesomeMarkers(~long, ~lat, icon=icons, popup =~price_category)

```

##### 7.1 Visual Representations (All Variables vs Price)
We will now visualize each numeric feature with the target variable to see how the price varies with the variation in the independent variables.
```{r Visualizing Variables with Price}
# We will now visualize all numeric columns against the house price
numeric_var <- names(house_train)[which(sapply(house_train, is.numeric))]
numeric_var

for (i in seq_along(numeric_var)) {
      plt <- ggplot(data = house_train, aes_string(numeric_var[i], 'price')) +
    geom_point(color = "dodgerblue3") +
      theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price')
  print(plt)
}


```

Now that we have a general idea about how the price varies with each variable, we will take each variable and analyze it separately with the price.
##### 7.2 Bedrooms
```{r Bedrooms}
table(house_dataset$bedrooms)
#From the above table we can see the majority of house has between 1-8 or 1-9 bedrooms and a few outliers having more than that

ggplot(house_dataset, aes(x=bedrooms)) + geom_histogram(fill="dodgerblue3", stat = "count") +
  xlab("Bedrooms") + ylab("Count") + theme_minimal() + labs(title = "Count vs # of Bedrooms") +
  scale_x_continuous(limits = c(0,9), breaks = 9)


#Now visualizing with the log of the price
ggplot(data = house_train, aes(bedrooms, log10(price))) +
    geom_point(color = "dodgerblue3") + theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Log_Price') + xlab('Bedrooms') + geom_smooth(method="lm",se=F, color = "red") +
    labs(title = "Log Price vs # of Bedrooms")

#We can notice the point of the house containing 33 bedrooms so we will plots the same graph again wihtout this outlier to adjust the linear fit
ggplot(data = house_train %>% filter(bedrooms<30), aes(bedrooms, log10(price))) +
    geom_point(color = "dodgerblue3") + theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Log_Price') + xlab('Bedrooms') + geom_smooth(method="lm",se=F, color = "red") +
    labs(title = "Log Price vs # of Bedrooms")

#To find the average price for the house based on the number of bedrooms 
bedrooms = house_train %>% filter(bedrooms <33) %>% group_by(bedrooms) %>% summarise(average_price = mean(price))
ggplot(bedrooms, aes(bedrooms, average_price)) + geom_col(fill = "dodgerblue3") + theme_minimal() +ylab('Average Price $') + xlab('Bedrooms')

getPalette = colorRampPalette(brewer.pal(8, "Spectral"))
colourCount <- 12

ggplot(house_train[house_train$bedrooms < 33,], aes(bedrooms,log10(price),fill=factor(bedrooms)))+
geom_boxplot(alpha=0.6)+scale_fill_manual(values=getPalette(colourCount))+ theme_minimal()+labs(x="Bedrooms", y="Log_Price", fill = "Bedrooms")

```

##### 7.3 Bathrooms
```{r Bathrooms}
table(house_dataset$bathrooms)
#From the above table we can see the houses have a range from 0.5 to 8 bathrooms
ggplot(house_dataset, aes(x=bathrooms)) + geom_bar(fill="dodgerblue3", stat = "count") +
  xlab("Bathrooms") + ylab("Count") + theme_minimal() + labs(title = "Count vs # of Bathrooms")


#Now visualizing with the log of the price
ggplot(data = house_train, aes(bathrooms, log10(price))) +
    geom_point(color = "dodgerblue3") + theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Log_Price') + xlab('Bathrooms') + geom_smooth(method="lm",se=F, color = "red") +
    labs(title = "Log Price vs # of Bathrooms")

#To find the average price for the house based on the number of bathrooms 
bathrooms = house_train %>% group_by(bathrooms) %>% summarise(average_price = mean(price))
ggplot(bathrooms, aes(bathrooms, average_price)) + geom_col(fill = "dodgerblue3") + theme_minimal() + ylab('Average Price $') + xlab('Bathrooms') +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

getPalette = colorRampPalette(brewer.pal(8, "Spectral"))
colourCount <- 29

ggplot(house_train,aes(bathrooms,log10(price),fill=factor(bathrooms)))+
geom_boxplot(alpha=0.6)+scale_fill_manual(values=getPalette(colourCount))+ theme_minimal()+labs(x="Bathrooms", y="Log_Price", fill = "Bathrooms")

```

##### 7.4 Floors
```{r Floors}
table(house_dataset$floors)

ggplot(house_dataset, aes(floors)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('Floors')

ggplot(house_train %>% group_by(floors) %>% summarise(average_price = mean(price)), aes(floors, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('Floors') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

#Now visualizing with the log of the price
ggplot(data = house_train, aes(floors, log10(price))) +
    geom_point(color = "dodgerblue3") + theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Log_Price') + xlab('Floors') + geom_smooth(method="lm",se=F, color = "red") +
    labs(title = "Log Price vs # of Floors")

getPalette = colorRampPalette(brewer.pal(8, "Spectral"))
colourCount <- 6

ggplot(house_train,aes(floors,log10(price),fill=factor(floors)))+
geom_boxplot(alpha=0.6)+scale_fill_manual(values=getPalette(colourCount))+ theme_minimal()+labs(x="Floors", y="Log_Price", fill = "Floors")

```


##### 7.5 House Condition
```{r House Condition}
table(house_dataset$condition)

ggplot(house_dataset, aes(condition)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('House Condition')

ggplot(house_train %>% group_by(condition) %>% summarise(average_price = mean(price)), aes(condition, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('Condition') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

ggplot(house_train,aes(condition,log10(price),fill=factor(condition)))+
geom_boxplot(alpha=0.6)+scale_fill_manual(values= getPalette(colourCount))+ theme_minimal()+labs(x="Condition", y="Log_Price", fill = "Condition")

```

##### 7.6 Water Front
```{r Water Front}
table(house_dataset$waterfront)
# Most of the houses in the dataset don't have a waterfront, only 163 of all the houses do.

ggplot(house_dataset, aes(waterfront)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('WaterFront')

ggplot(house_train %>% group_by(waterfront) %>% summarise(average_price = mean(price)), aes(waterfront, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('WaterFront') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

ggplot(house_train,aes(waterfront,log10(price),fill=factor(waterfront)))+
geom_boxplot(alpha=0.6)+scale_fill_manual(values= getPalette(colourCount))+ theme_minimal()+labs(x="WaterFront", y="Log_Price", fill = "WaterFront")
```

##### 7.7 Sqft_living
```{r Sqft_living}

ggplot(data = house_train, aes(sqft_living, price)) +
    geom_point(color = "dodgerblue3") +
    theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price') +
    geom_smooth(method="lm",se=F, color = "red")

```

##### 7.8 Sqft_living15
```{r Sqft_living15}
# Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area

ggplot(data = house_train, aes(sqft_living15, price)) +
    geom_point(color = "dodgerblue3") +
    theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price') +
    geom_smooth(method="lm",se=F, color = "red")

```

##### 7.9 Sqft_lot
```{r Sqft_lot}

ggplot(data = house_train, aes(sqft_lot, price)) +
    geom_point(color = "dodgerblue3") +
    theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price') +
    geom_smooth(method="lm",se=F, color = "red")

```

##### 7.10 Sqft_lot15
```{r Sqft_lot15}

# Sqft_lot15 is the lot area in 2015 (implies-- some renovations) This might or might not have affected the lotsize area

ggplot(data = house_train, aes(sqft_lot15, price)) +
    geom_point(color = "dodgerblue3") +
    theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price') +
    geom_smooth(method="lm",se=F, color = "red")

```

##### 7.11 Sqft_above
```{r Sqft_above}

ggplot(data = house_train, aes(sqft_above, price)) +
    geom_point(color = "dodgerblue3") +
    theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price') +
    geom_smooth(method="lm",se=F, color = "red")

```

##### 7.12 Sqft_basement
```{r Sqft_basement}

ggplot(data = house_train, aes(sqft_basement, price)) +
    geom_point(color = "dodgerblue3") +
    theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Price') +
    geom_smooth(method="lm",se=F, color = "red")

```

##### 7.13 Views
```{r Views}

#How many times the house has been viewed
table(house_dataset$view)

ggplot(house_dataset, aes(view)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('View')
# Most of the houses have not been viewed

ggplot(house_train %>% group_by(view) %>% summarise(average_price = mean(price)), aes(view, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('View') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

```

##### 7.14 Grade
```{r Grade}

table(house_dataset$grade)

ggplot(house_dataset, aes(grade)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('Grade')

ggplot(house_train %>% group_by(grade) %>% summarise(average_price = mean(price)), aes(grade, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('Grade') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

#Now visualizing with the log of the price
ggplot(data = house_train, aes(grade, log10(price))) +
    geom_point(color = "dodgerblue3") + theme_minimal() +
    scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + 
    scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + ylab('Log_Price') + xlab('Grade') + geom_smooth(method="lm",se=F, color = "red") +
    labs(title = "Log Price vs # of Floors")

colourCount = 11

ggplot(house_train,aes(grade,log10(price),fill=factor(grade)))+
geom_boxplot(alpha=0.6)+scale_fill_manual(values= getPalette(colourCount))+ theme_minimal()+labs(x="Grade", y="Log_Price", fill = "Grade")

```

##### 7.15 Year Built
```{r Year Built}
table(house_dataset$yr_built)

# Since there is a huge variation in the year build variable, we can bin this feature to visualize the count. 
yr_bin = house_dataset %>% mutate(year_built_bin = ifelse(yr_built >= 1900 & yr_built < 1910, "1900s",
                                                 ifelse(yr_built >= 1910 & yr_built < 1920, "1910s",
                                                        ifelse(yr_built >= 1920 & yr_built < 1930, "1920s",
                                                               ifelse(yr_built >= 1930 & yr_built < 1940, "1930s",
                                                                      ifelse(yr_built >= 1940 & yr_built < 1950, "1940s",
                                                                             ifelse(yr_built >= 1950 & yr_built < 1960, "1950s",
                                                                                    ifelse(yr_built >= 1960 & yr_built < 1970, "1960s",
                                                                                           ifelse(yr_built >= 1970 & yr_built < 1980, "1970s",
                                                                                                  ifelse(yr_built >= 1980 & yr_built < 1990, "1980s",
                                                                                                         ifelse(yr_built >= 1990 & yr_built < 2000, "1990s",
                                                                                                                ifelse(yr_built >= 2000, "2000s", NA))))))))))))

ggplot(yr_bin, aes(year_built_bin)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('Year Built')


ggplot(yr_bin[1:17227,] %>% group_by(year_built_bin) %>% summarise(average_price = mean(price)), aes(year_built_bin, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('Year Built') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

```

##### 7.16 Year Renovated
```{r Year Renovated}
table(house_dataset$yr_renovated)

# Since there is a huge variation in the year build variable, we can bin this feature to visualize the count. 
yr_ren_bin = house_dataset %>% mutate(year_ren_bin = ifelse(yr_renovated >= 1900 & yr_renovated < 1910, "1900s",
                                                 ifelse(yr_renovated >= 1910 & yr_renovated < 1920, "1910s",
                                                        ifelse(yr_renovated >= 1920 & yr_renovated < 1930, "1920s",
                                                               ifelse(yr_renovated >= 1930 & yr_renovated < 1940, "1930s",
                                                                      ifelse(yr_renovated >= 1940 & yr_renovated < 1950, "1940s",
                                                                             ifelse(yr_renovated >= 1950 & yr_renovated < 1960, "1950s",
                                                                                    ifelse(yr_renovated >= 1960 & yr_renovated < 1970, "1960s",
                                                                                           ifelse(yr_renovated >= 1970 & yr_renovated < 1980, "1970s",
                                                                                                  ifelse(yr_renovated >= 1980 & yr_renovated < 1990, "1980s",
                                                                                                         ifelse(yr_renovated >= 1990 & yr_renovated < 2000, "1990s",
                                                                                                                ifelse(yr_renovated >= 2000, "2000s", NA))))))))))))

ggplot(yr_ren_bin, aes(year_ren_bin)) + geom_bar(fill = "dodgerblue3") + theme_minimal() +ylab('Count') + xlab('Year Renovated')


ggplot(yr_ren_bin[1:17227,] %>% group_by(year_ren_bin) %>% summarise(average_price = mean(price)), aes(year_ren_bin, average_price)) + geom_col(fill = "dodgerblue3") + ylab('Average Price $') + xlab('Year Renovated') + scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

```

##### 7.17 House Price Binning
```{r}
quantile(house_train$price)
# We can bin the prices using the above quantiles or using to selected ranges visualize counts per bin

# Lets look at the price distribution again
ggplot(house_train,aes(x=price))+geom_histogram(fill="dodgerblue3") + scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
  theme_minimal() + xlab('Price') + ylab('Count')

# Most houses lie in the range below 1,000,000
house_dataset <- house_dataset %>% mutate(price_bin = ifelse(price < 500000, "Less than 500000",
                                                             ifelse(price >= 500000 & price < 750000, "Between 500000 & 750000",
                                                                    ifelse(price >= 750000 & price < 1000000, "Between 750000 & 1000000",
                                                                           ifelse(price >= 1000000 & price < 2000000, "Between 1 & 2 Million",
                                                                                  ifelse(price >= 2000000, "More than 2 Million", NA))))))

ggplot(house_dataset[house_dataset$train_test == "train",], aes(price_bin)) + geom_bar(fill = "dodgerblue3") +
  theme_minimal() +ylab('Count') + xlab('Price Bin') + theme(axis.text.x = element_text(angle = 18, hjust = 1, vjust = 0.5)) + 
  scale_x_discrete(limits=c("Less than 500000", "Between 500000 & 750000", "Between 750000 & 1000000", "Between 1 & 2 Million", "More than 2 Million"))

```

#### 8. Feature Engineering/Creation
In this section we will do some variable transformation and variable creation to improve our final results. Some variables will be created however will be eliminated later by our regularization model as we apply penalties using whether Lasso or Ridge regressions 
```{r}

house_dataset$total_sf15 <- house_dataset$sqft_living15 + house_dataset$sqft_lot15

# We will extract the year the house was sold from the date
house_dataset$date <- as.character(house_dataset$date)
house_dataset = house_dataset %>% mutate(YearSold = str_sub(date, -4, -1))
house_dataset$YearSold <- as.numeric(house_dataset$YearSold)


# creating a binary feature if the house has a basement or not
house_dataset <- house_dataset %>% mutate(has_basement = ifelse(sqft_basement == 0, 0, 1))
house_dataset$has_basement <- as.factor(house_dataset$has_basement)

# We will create a dummy variable NewHouse using the YearSold and YearBuild. If the two values are equal then it's a new house and will be evaluated as 1 otherwise it will be 0
house_dataset$NewHouse <- ifelse(house_dataset$YearSold == house_dataset$yr_built, 1, 0)
table(house_dataset$NewHouse) #430 New Houses
house_dataset$NewHouse <- as.factor(house_dataset$NewHouse)

#We will check the age of the house. If the year of rennovation is 0 then it means that the house has not been rennovated and the age will be the YearSold - yr_build, 
#If the year of rennovation is present, then the age will be the year sold minus the year of rennovation
house_dataset <- house_dataset %>% mutate(Age = ifelse(yr_renovated != 0, YearSold - yr_renovated, YearSold - yr_built))

```

#### 9. Correlations
Correlations are used to identify which variable has more correlation with our target variable. We will explore correlations using a heat map for a better understanding of the data. Again, it is important to note that some categorical features correlations are calculated within the heat map due to their numeric nature. So we will not take them into consideration.

```{r}
#create a set of numeric variables to see correlations with the sale price
numeric_var <- names(house_dataset[house_dataset$train_test == "train",])[which(sapply(house_dataset[house_dataset$train_test == "train",], is.numeric))]
cat_var <- names(house_dataset[house_dataset$train_test == "train",])[which(sapply(house_dataset[house_dataset$train_test == "train",], is.factor))]

train_num <- house_dataset[house_dataset$train_test == "train",][numeric_var]

num_correlations <- cor(train_num[,])
corrplot(num_correlations, method = "color")
corrplot(num_correlations, method = "pie")

```

#### 10. Outliers
Outliers represent a big problem for numerical variables and can decrease the quality of any model and cause bias in our predeictions. We will look at numerical variables manually using ggplot and remove outliers accordingly. I tried to use a for loop to remove outliers however many values ended up being removed so I choose some features manually and removed outliers visually.

Train and test data will be splits for outlier removal then we will bind them again for further preprocessing.
It is also important to note than not all columns with numeric values are classified as numeric variables. Some variables represent quality for example.
```{r Outlier Detection and Removal}

train <- house_dataset[house_dataset$train_test == 'train',]
test <- house_dataset[house_dataset$train_test == 'test',]


par(mfrow=c(2,5))
for (col in names(train)) {
  if (is.numeric(train[[col]]) ){
    boxplot(train[,col], main=names(train[col]), type="l", col = "red")
  }
}

if(FALSE){ #-> removes too many rows
  
  nrow(train)
  
  for (col in names(train)) {
    if (!is.factor(train[[col]])){
      print(ggplot(train, aes_string(y=col)) + 
              geom_boxplot(width = 0.1, outlier.size = 5) + 
              theme(axis.line.x = element_blank(), axis.title.x = element_blank(), 
                    axis.ticks.x = element_blank(), axis.text.x = element_blank(),
                    legend.position="none"))
      
      to_remove <- boxplot.stats(train[[col]])$out
      train <- train[!train[[col]] %in% to_remove, ]
    }
  }
  
  nrow(train) 
}

nrow(train)

#Since the too many rows are deleted, we will comment it out and rather remove outliers manually by looking at the visualizations for each variable against the target variable

train <- subset(train, bedrooms < 9)
train <- subset(train, bathrooms <= 6)
train <- subset(train, floors <= 3.5)
train <- subset(train, sqft_living < 8000)
train <- subset(train, sqft_lot < 750000)
train <- subset(train, sqft_lot15 < 375000)
train <- subset(train, sqft_above < 5500)
train <- subset(train, sqft_basement < 3000)

# We can also see that our house prices lie in a lower range of prices. Some of the houses have a really high price than most in the dataset so we will remove those so that they dont affect our model
# train <- subset(train, price < 4000000)

nrow(train)
# Combining train and test again
house_dataset <- rbind(train, test)


```
#### 11. Skewness
```{r}
# Adjusting Skewness in Target Variable
# Histogram to see the distribution of the house prices
ggplot(house_dataset,aes(x=price))+geom_density(fill="dodgerblue3") + scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()

house_dataset$price <- log1p(house_dataset$price)

# Visualizing Target Variable again
ggplot(house_dataset,aes(x=price))+geom_density(fill="dodgerblue3") + scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) + theme_minimal()
```

#### 12. Factorizing Features
There are some numerical features that should be perseved as categorical ones, in order to gasp correctly the information inside those columns.
If we analyze some features we will see that we have categorical values encoded as numeric. We will transform some of those into factors since they cant be used in any calculation.
```{r}
#### Factorize the remaining features
house_dataset$yr_built <- as.factor(house_dataset$yr_built)
house_dataset$yr_renovated <- as.factor(house_dataset$yr_renovated)
house_dataset$YearSold <- as.factor(house_dataset$YearSold)
house_dataset$train_test <- as.factor(house_dataset$train_test)
house_dataset$price_bin <- as.factor(house_dataset$price_bin)
house_dataset$view <- as.factor(house_dataset$view)
house_dataset$bedrooms <- as.factor(house_dataset$bedrooms)
house_dataset$bathrooms <- as.factor(house_dataset$bathrooms)
house_dataset$floors <- as.factor(house_dataset$floors)
#house_dataset$lat <- as.factor(house_dataset$lat)
#house_dataset$long <- as.factor(house_dataset$long)

#Removing Features
house_dataset <- house_dataset[,-which(names(house_dataset) == "date")]
#house_dataset <- house_dataset[,-which(names(house_dataset) == "lat")]
# house_dataset <- house_dataset[,-which(names(house_dataset) == "long")]
# house_dataset <- house_dataset[,-which(names(house_dataset) == "yr_built")]
house_dataset <- house_dataset[,-which(names(house_dataset) == "yr_renovated")]
house_dataset <- house_dataset[,-which(names(house_dataset) == "YearSold")]
house_dataset <- house_dataset[,-which(names(house_dataset) == "price_bin")]

```

#### 13. Scaling
We will now standardize all the numeric features since the range of the data values varies widely. We will scale the variables so that their scale varies around zero. This is done so that the variance of the features are in the same range. If a feature's variance in orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset, which is not something we want happening in our model.
```{r}
# Feature scaling
numericFeatures = sapply(house_dataset[,-1], is.numeric)
numericFeatures = c(FALSE, numericFeatures) # No need to scale price
house_dataset[numericFeatures] = sapply(house_dataset[numericFeatures], scale)
```

#### 14. More Feature Engineering
We will now create bins for both longitude and latitude since some models dont accept factors more than 53 categories. Therefore some of these bins will be used later in some models and for others we will normally use longitude and latitude.
```{r}
max(house_dataset$long) #-> 6.395034
min(house_dataset$long) #->-2.168754


house_dataset <- house_dataset %>% mutate(long_bin = ifelse(long >= -2.91653 & long < -1, 'long_1',
                                                            ifelse(long >= -1 & long < 0, 'long_2',
                                                                   ifelse(long >= 0 & long < 1, 'long_3',
                                                                          ifelse(long >= 1 & long < 2, 'long_4',
                                                                                 ifelse(long>=2&long<3, 'long_5',
                                                                                        ifelse(long>=3 & long<4,'long_6',
                                                                                               ifelse(long>=4&long<5, 'long_7',
                                                                                                      ifelse(long>=5,'long_8',NA)))))))))

max(house_dataset$lat) #-> 1.569772
min(house_dataset$lat) #-> -2.916866


house_dataset <- house_dataset %>% mutate(lat_bin = ifelse(lat >= -2.916532 & lat < -2.3, 'lat_1',
                                                            ifelse(lat >= -2.3 & lat < -1.6, 'lat_2',
                                                                   ifelse(lat >= -1.6 & lat < -1, 'lat_3',
                                                                          ifelse(lat >= -1 & lat < -0.4, 'lat_4',
                                                                                 ifelse(lat>= -0.4&lat<0.1, 'lat_5',
                                                                                        ifelse(lat>= 0.1 & lat<0.6,'lat_6',
                                                                                               ifelse(lat>=0.6&lat<1.1, 'lat_7',
                                                                                                      ifelse(lat>=1.1,'lat_8',NA)))))))))

house_dataset$long_bin <- as.factor(house_dataset$long_bin)
house_dataset$lat_bin <- as.factor(house_dataset$lat_bin)

```

#### 15. Data split
To facilitate the data cleaning and feature engineering we merged train and test datasets. We now split them again to create our final model.
```{r Data split}
## Train / Test split
# Now we split the dataset again for model training
train_data <- house_dataset[house_dataset$train_test == "train",]
test_data <- house_dataset[house_dataset$train_test == "test",]

dim(train_data)
dim(test_data)

train_data <- train_data[,-which(names(train_data) == "train_test")]
test_data <- test_data[,-which(names(test_data) == "train_test")]

```

#### 15.1 Train / Validation split
We are going to split the annotated dataset in training and validation for the later evaluation of our regression models
```{r Data Split Function}
# Function to split a dataset into training and validation.
splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.2))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}

# Applying the function to the training_data
splits <- splitdf(train_data, seed = 8)

# Splitting the data into train set and validation set
training <- splits$trainset
validation <- splits$testset

dim(training)
dim(validation)

```

#### 16. Modeling
##### 16.1 Regression Metrics
First we will define three metric functions to measure the strength and accuracy of our model.
```{r Metric Functions}

# Mean Absolute Error
mae<-function(real, predicted){
  return(mean(abs(real-predicted)))
}

# Mean Absolute Percentage Error
mape<-function(real,predicted){
  return(mean(abs((real-predicted)/real)))
}

# Root Mean Squared Error
rmse<-function(real,predicted){
  return(sqrt(mean((real-predicted)^2)))
}

```

##### 16.2 Linear Regression
We will now start with a baseline linear model to evaluate our prediction and further we will use regularization methods to do feature elimination and optimize our results

```{r Linear Regression}
training$bathrooms <- as.factor(training$bathrooms)
validation$bathrooms <- as.factor(validation$bathrooms)
training$bedrooms <- as.factor(training$bedrooms)
validation$bedrooms <- as.factor(validation$bedrooms)

lm_model = lm(formula = price ~  bedrooms + bathrooms + sqft_living + sqft_lot + sqft_above + floors + waterfront + view + condition + grade + sqft_basement + zipcode + sqft_living15 + sqft_lot15 + yr_built + NewHouse + Age + long + lat + has_basement,
           data = training)
```

###### 16.2.1 Evaluation
```{r}
# Summary of the model
summary(lm_model)

mean(lm_model$residuals)
hist(lm_model$residuals, main = 'Residuals Distribution', col = "dodgerblue3")
# Residuals are distributed around zero!

validation$pred <- predict(lm_model,validation)
lm_mae <- mae(real = validation$price, predicted = validation$pred)
lm_mape <- mape(real = validation$price, predicted = validation$pred)
lm_rmse <- rmse(real = validation$price, predicted = validation$pred)

paste("The mean absolute error for linear regression is ", lm_mae, "and mape is", lm_mape, "and rmse is ", lm_rmse)

# Since the price was log transformed we will recompute the error metrics by reversing the log transformation
lm_mae <- mae(real = expm1(validation$price), predicted = expm1(validation$pred))
lm_mape <- mape(real = expm1(validation$price), predicted = expm1(validation$pred))
lm_rmse <- rmse(real = expm1(validation$price), predicted = expm1(validation$pred))

paste("The mean absolute error for linear regression is ", lm_mae, "and mape is", lm_mape, "and rmse is ", lm_rmse)

# Plotting predictions vs real
ggplot(validation,aes(x=expm1(price),y=expm1(pred)))+geom_point()+geom_smooth(method = "lm", col = 'red') + theme_minimal() + labs(x="Real", y="Prediction") +
  ggtitle(paste('RMSE:', format(round(lm_rmse, 4)), " and MAPE -->", format(round(lm_mape, 4)))) + labs(subtitle = paste(" and Mean Price Error:", format(round(lm_mae,4)))) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

paste("R-Squared:", summary(lm_model)$r.squared)

```

Now that we have an intial regression model, we can start working to improve our prediction by practicing regularization. Regularization methods add a penalty on the variables and lead the model into having fewer coefficients. These methods are Lasso and Ridge regressions.

Ridge and Lasso Regressions are similar in terms of methodology, the superficial difference between both models is that Ridge squares the variables while Lasso regression uses the absolute value.Though, Lasso is  better than ridge regression at reducing the variance in models that contain a lot of useless variables, so in the context of our dataset, Lasso will be very beneficial in improving predictions.

In summary, using Ridge regression and Lasso regression both helps with variable elimination however Lasso is mainly useful for excluding useless variables from our regression equation which makes the final prediction equation simpler. On the other hand Ridge will be useful when most of our features are good for the prediction.

##### 16.3 Ridge Regression
The alpha parameter represents the penalty in the regularization process. For Ridge regression, alpha value of 0 is used while in Lasso we use an alpha value of 1.
```{r Ridge Regression, warning=FALSE}
lambdas <- 10^seq(-4, 0, by = .05)

set.seed(121)
my_control <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge_model <- train(price ~ ., data = training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=my_control,
               tuneGrid = expand.grid(alpha = 0, lambda = lambdas))
```


###### 16.3.1 Ridge Evaluation
Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.
Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. However, as we can see at the top of the features, there is no feature selection.

```{r Ridge Evaluation}
summary(ridge_model)
plot(ridge_model)
plot(ridge_model$finalModel)

validation$ridge_pred <- predict(ridge_model,validation)
# Since the price was log transformed we will recompute the error metrics by reversing the log transformation
ridge_mae <- mae(real = expm1(validation$price), predicted = expm1(validation$ridge_pred))
ridge_mape <- mape(real = expm1(validation$price), predicted = expm1(validation$ridge_pred))
ridge_rmse <- rmse(real = expm1(validation$price), predicted = expm1(validation$ridge_pred))

paste("The mean absolute error for Ridge regression is ", ridge_mae, "and mape is", ridge_mape, "and rmse is ", ridge_rmse)

# Print, plot variable importance
plot(varImp(ridge_model), top = 20) # 20 most important features

# Plotting predictions vs real
ggplot(validation,aes(x=expm1(price),y=expm1(ridge_pred)))+geom_point()+geom_smooth(method = "lm", col = 'red') + theme_minimal() + labs(x="Real", y="Prediction") + ggtitle(paste("Ridge Regression -->",'RMSE:', format(round(ridge_rmse, 4)), " and MAPE -->", format(round(ridge_mape, 4)))) + labs(subtitle = paste(" and Mean Price Error:", format(round(ridge_mae,4)))) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

# Sum of Squares Total and Error
sst_ridge <- sum((validation$price - mean(validation$price))^2)
sse_ridge <- sum((validation$ridge_pred - validation$price)^2)

# R-squared
r2_ridge <- 1- sse_ridge/sst_ridge
paste("Ridge R-Squared:", r2_ridge)

```

##### 16.4 Lasso Regression
The only thing that changes between Lasso and Ridge is the alpha parameter. We will use the same sequence of lambda values used in ridge and apply lasso by changing alpha into 1. a value of 1 for alpha means that Lasso applies a higher penalty in the regularization more than in Ridge.
```{r Lasso Regression, warning=FALSE}
set.seed(121)
my_control <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

lasso_model <- train(price ~ ., data = training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=my_control,
               tuneGrid = expand.grid(alpha = 1, lambda = lambdas))
```

###### 16.4.1 Lasso Evaluation
```{r Lasso Evaluation}
summary(lasso_model)
plot(lasso_model)
plot(lasso_model$finalModel)

validation$lasso_pred <- predict(lasso_model,validation)
# Since the price was log transformed we will recompute the error metrics by reversing the log transformation
lasso_mae <- mae(real = expm1(validation$price), predicted = expm1(validation$lasso_pred))
lasso_mape <- mape(real = expm1(validation$price), predicted = expm1(validation$lasso_pred))
lasso_rmse <- rmse(real = expm1(validation$price), predicted = expm1(validation$lasso_pred))

paste("The mean absolute error for Lasso regression is ", lasso_mae, "and mape is", lasso_mape, "and rmse is ", lasso_rmse)

# Print, plot variable importance
plot(varImp(lasso_model), top = 20) # 20 most important features

# Plotting predictions vs real
ggplot(validation,aes(x=expm1(price),y=expm1(lasso_pred)))+geom_point()+geom_smooth(method = "lm", col= 'red') + theme_minimal() + labs(x="Real", y="Prediction") +
  ggtitle(paste("Lasso Regression -->",'RMSE:', format(round(lasso_rmse, 4)), " and MAPE -->", format(round(lasso_mape, 4)))) +
  labs(subtitle = paste(" and Mean Price Error:", format(round(lasso_mae,4)))) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

# Sum of Squares Total and Error
sst_lasso <- sum((validation$price - mean(validation$price))^2)
sse_lasso <- sum((validation$lasso_pred - validation$price)^2)

# R-squared
r2_lasso <- 1- sse_lasso/sst_lasso
paste("Lasso R-Squared:", r2_lasso)
```

##### 16.5 Defining Formulas For XGB & RF
Defining Formula to be used for the following models. We will definte two formulas and they will be used differently based on the model. Next we will try using XGBoost and Random Forest models and we will check for improvement.
```{r}
# we start defining a formula
formula1 <-as.formula(price ~  bedrooms + bathrooms + sqft_living + sqft_lot + sqft_above + floors + waterfront + view + condition + grade
             + sqft_basement + sqft_living15 + sqft_lot15 + NewHouse + Age + long + lat)

formula2 <- as.formula(price ~.)

```

##### 16.6 XGBoost
XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. We will try using XGBoost with grid search using the parameters below. The initial grid search was performed and best parameters were selected for identifying the grid parameters. However, for convenience I commented out the grid search part for faster evaluation and knitting processes.
```{r}

my_control <- trainControl(method = "cv", number = 3)

# xgbGrid <- expand.grid(nrounds = 1000,
#                        max_depth = c(12,15),
#                        eta = c(0.05, 0.1),
#                        gamma = c(0, 0.2, 1),
#                        colsample_bytree = 0.7,
#                        min_child_weight = 3, subsample = c(0.5, 1))

xgbGrid <- expand.grid(nrounds = 1500,
                       max_depth = 15,
                       eta = 0.05,
                       gamma = 0.2,
                       colsample_bytree = 0.7,
                       min_child_weight = 1,
                       subsample = 0.5)

xgb = train(formula2, data = training,
                   method = "xgbTree",trControl = my_control,
                   tuneGrid = xgbGrid,na.action = na.pass,metric="RMSE")


```

###### 16.6.1 XGBoost Evaluation
```{r}
summary(xgb)

validation$xgb_pred <- predict(xgb,validation)
# Since the price was log transformed we will recompute the error metrics by reversing the log transformation
xgb_mae <- mae(real = expm1(validation$price), predicted = expm1(validation$xgb_pred))
xgb_mape <- mape(real = expm1(validation$price), predicted = expm1(validation$xgb_pred))
xgb_rmse <- rmse(real = expm1(validation$price), predicted = expm1(validation$xgb_pred))

paste("The mean absolute error for xgboost is ", xgb_mae, "and mape is", xgb_mape, "and rmse is ", xgb_rmse)

plot(varImp(xgb), top = 20)

# Plotting predictions vs real
ggplot(validation,aes(x=expm1(price),y=expm1(xgb_pred)))+geom_point()+geom_smooth(method = "lm", col = 'red') + theme_minimal() + labs(x="Real", y="Prediction") +
  ggtitle(paste("XGBoost Regression -->",'RMSE:', format(round(xgb_rmse, 4)), " and MAPE -->", format(round(xgb_mape, 4)))) +
  labs(subtitle = paste(" and Mean Price Error:", format(round(xgb_mae,4)))) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

# Sum of Squares Total and Error
sst_xgb <- sum((validation$price - mean(validation$price))^2)
sse_xgb <- sum((validation$xgb_pred - validation$price)^2)

# R-squared
r2_xgb <- 1- sse_xgb/sst_xgb
paste("XGB R-Squared:", r2_xgb)
```

##### 16.7 Random Forest
Since RF does not accept factors more than 53 levels we will remove factors with more than 53 levels
Therefore, we will use a separate dataset where we convert zipcode into a numeric variable and create a new formula with hand-picked variables to be used for the the model.
```{r Random Forest Base Model}

rftraining <- training
rfvalidation <- validation

rftraining$zipcode <- as.numeric(rftraining$zipcode)
rftraining$yr_built <- as.numeric(rftraining$yr_built)
rfvalidation$zipcode <- as.numeric(rfvalidation$zipcode)
rfvalidation$yr_built <- as.numeric(rfvalidation$yr_built)

str(rftraining)

rffomula <- as.formula(price ~bedrooms + bathrooms + sqft_living + sqft_lot + sqft_above + floors + waterfront + view + condition + grade + sqft_basement + zipcode + sqft_living15 + sqft_lot15 + yr_built + NewHouse + Age + long + lat + zipcode)

rf_0<-randomForest(formula=rffomula, data=rftraining, keep.forest = TRUE, keep.inbag = TRUE)
print(rf_0)

model_imp <- as.data.frame(rf_0$importance, stringsAsFactors = FALSE)
model_imp$IncNodePurity <- round(model_imp$IncNodePurity, 2)
model_imp

```

###### 16.7.1 Random Forest Evaluation
```{r RF Evaluation}
summary(rf_0)
plot(rf_0)

rfvalidation$rf0_pred <- predict(rf_0,rfvalidation)

# Since the price was log transformed we will recompute the error metrics by reversing the log transformation
rf0_mae <- mae(real = expm1(rfvalidation$price), predicted = expm1(rfvalidation$rf0_pred))
rf0_mape <- mape(real = expm1(rfvalidation$price), predicted = expm1(rfvalidation$rf0_pred))
rf0_rmse <- rmse(real = expm1(rfvalidation$price), predicted = expm1(rfvalidation$rf0_pred))

paste("The mean absolute error for Random Forest is ", rf0_mae, "and mape is", rf0_mape, "and rmse is ", rf0_rmse)

# Plotting predictions vs real
ggplot(rfvalidation,aes(x=expm1(price),y=expm1(rf0_pred)))+geom_point()+geom_smooth(method = "lm", col = 'red') + theme_minimal() + labs(x="Real", y="Prediction") +
  ggtitle(paste("Random Forest -->",'RMSE:', format(round(rf0_rmse, 4)), " and MAPE -->", format(round(rf0_mape, 4)))) +
  labs(subtitle = paste(" and Mean Price Error:", format(round(rf0_mae,4)))) +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))# Sum of Squares Total and Error

sst_rf <- sum((rfvalidation$price - mean(rfvalidation$price))^2)
sse_rf <- sum((rfvalidation$rf0_pred - rfvalidation$price)^2)

# R-squared
r2_rf <- 1- sse_rf/sst_rf
paste("Random Forest R-Squared:", r2_rf)

```

###### 16.7.2 Random Forest Tuning
```{r Random Forest Tuning, eval=FALSE}
# tuneGrid=data.table(expand.grid(mtry=c(5,15),
#                                 splitrule='variance',
#                                 min.node.size=c(2,5,10)))

tuneGrid=data.table(expand.grid(mtry=c(15),
                                splitrule='variance',
                                min.node.size=c(2,5,10)))

train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")
# 3. Train the model
set.seed(123)

rf_tune <- train(
  formula,
  data = rftraining,
  method = "ranger", num.trees=700,
  preProc = NULL, 
  tuneGrid = tuneGrid,
  trControl = train_control_config,
  metric = "MAE"
)

rf_tune

# inspecting the most relevant features: 
rf_tune$results
rf_tune$bestTune
rf_tune$finalModel

# and visualize a CV summary using the established metric
plot(rf_tune)

rf_tuned<-ranger(formula, data=rftraining, num.trees=500,
                   mtry=rf_tune$bestTune$mtry,
                   min.node.size=rf_tune$bestTune$min.node.size)

```

After hyper-parameter optimization, model evaluation has not revealed any improvement in the original model. Therefore, XGBoost remains the best score of all implemented models.

###### 16.7.3 Optimized Random Forest Evaluation
```{r Optimized RF Evaluation, eval=FALSE}

# rft_pred <- predict(rf_tuned, validation)
# 
# mape(real=validation$price, predicted = as.numeric(rft_pred))
# qplot(test_rf, test_rf1)
# 
# 
# # Since the price was log transformed we will recompute the error metrics by reversing the log transformation
# rft_mae <- mae(real = expm1(validation$price), predicted = expm1(validation$rft_pred))
# rft_mape <- mape(real = expm1(validation$price), predicted = expm1(validation$rft_pred))
# rft_rmse <- rmse(real = expm1(validation$price), predicted = expm1(validation$rft_pred))
# 
# paste("The mean absolute error for linear regression is ", rft_mae, "and mape is", rft_mape, "and rmse is ", rft_rmse)
# 
# # Plotting predictions vs real
# ggplot(validation,aes(x=expm1(price),y=expm1(rft_pred)))+geom_point()+geom_smooth(method = "lm") + theme_minimal() + labs(x="Real", y="Prediction") +
#   ggtitle(paste("Optimized Random Forest -->",'RMSE:', format(round(rft_rmse, 4)), " and Mean Price Error:", format(round(rft_mae,4)))) +
#   scale_x_continuous(labels = function(x) format(x, scientific = FALSE)) +
#   scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

```

##### 17. Model Comparison
Now we will compare all the implemented models based on all MAE, MAPE, RMSE, and R-squared values. The best model will be used to predict the test set.
```{r}

paste("The mean absolute error for linear regression is ", lm_mae, "and mape is", lm_mape, "and rmse is ", lm_rmse)
paste("Linear Regression R-Squared:", summary(lm_model)$r.squared)

paste("The mean absolute error for ridge regression is ", ridge_mae, "and mape is", ridge_mape, "and rmse is ", ridge_rmse)
paste("Ridge R-Squared:", r2_ridge)

paste("The mean absolute error for lasso regression is ", lasso_mae, "and mape is", lasso_mape, "and rmse is ", lasso_rmse)
paste("Lasso R-Squared:", r2_lasso)

paste("The mean absolute error for xgboost regression is ", xgb_mae, "and mape is", xgb_mape, "and rmse is ", xgb_rmse)
paste("XGBoost R-Squared:", r2_xgb)

paste("The mean absolute error for random forest regression is ", rf0_mae, "and mape is", rf0_mape, "and rmse is ", rf0_rmse)
paste("Random Forest R-Squared:", r2_rf)

```

##### 18. Test Prediction
Now we will predict the test set using the best model. XGBoost gave the lowest mean absolute percentage error of 0.114.
```{r}
test_data$price <- predict(xgb, test_data)

# Attach prediction to the original dataset
original_test$price <- expm1(test_data$price)

submission_columns <- c("id", "price")

submission <- original_test[, names(original_test) %in% submission_columns]

write.csv(submission, file = 'submission.csv', row.names = FALSE)

```